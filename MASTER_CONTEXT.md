# üéØ JOB SCRAPER - MASTER CONTEXT
**Complete System Reference for AI Agents & Developers**  
**Version:** 1.0.0  
**Created:** 2026-02-09  
**Purpose:** Single source of truth consolidating all documentation

---

## üìë DOCUMENT MAP - START HERE

### Which Section Do I Need?

| Your Need | Go To Section | Est. Read Time |
|-----------|---------------|----------------|
| **Brand new to this project?** | [5-Minute Quick Start](#5-minute-quick-start) | 5 min |
| **Need to verify system health?** | [System Health Check](#system-health-check) | 2 min |
| **Understanding architecture?** | [System Architecture](#system-architecture) | 10 min |
| **Looking up a function?** | [Appendix A: Function Inventory](#appendix-a-function-inventory) | Lookup |
| **Configuring settings?** | [Appendix B: Config Reference](#appendix-b-config-reference) | 5 min |
| **Running the scraper?** | [Running Jobs](#running-jobs) | 5 min |
| **Debugging errors?** | [Troubleshooting](#troubleshooting) | 10 min |
| **Adding new features?** | [Development Guide](#development-guide) | 15 min |
| **Fixing LinkedIn/Seek/Jora issues?** | [Appendix D: Verified Selectors](#appendix-d-verified-selectors) | Lookup |

### Original Documentation Archive

These files are **SUPERSEDED** by this master context (archived to `archive/documentation/`):
- ~~CODEBASE_REFERENCE.md~~ ‚Üí Now in [Technical Reference](#technical-reference) + [Appendix A](#appendix-a-function-inventory)
- ~~SYSTEM_STATUS.md~~ ‚Üí Now in [Protected Components](#protected-components) + [Appendix D](#appendix-d-verified-selectors)
- ~~ARCHITECTURE.md~~ ‚Üí Now in [System Architecture](#system-architecture) + [Planning & Roadmap](#planning--roadmap)
- ~~PRE_FLIGHT_CHECKLIST.md~~ ‚Üí Now in [Testing Guide](#testing-guide)

**Keep these files** (serve different purposes):
- ‚úÖ README.md - User-facing marketing/quick start (links here for details)
- ‚úÖ CONTRIBUTING.md - Git workflow and PR process
- ‚úÖ .github/copilot-instructions.md - AI-specific coding patterns

---

## üöÄ 5-MINUTE QUICK START
**Last Updated:** 2026-02-09 | **Verified:** All commands tested

### What This System Does

Multi-source job scraper that:
1. **Scrapes** jobs from LinkedIn, Seek, Jora (100-500 jobs/day)
2. **Filters** with 3-tier optimization (30% efficiency gain)
3. **Scores** with AI (Claude/GPT-4/Gemini) matching your profile
4. **Notifies** via email for jobs scoring 70%+
5. **Dashboard** for viewing, rejecting, tracking applications

### Essential Commands

```bash
# 1. First-time setup (run once)
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 2. Configure (edit these files)
cp config.json.example config.json  # Add your API keys
nano profile.txt                    # Your resume/profile
nano jobs.txt                       # Target job roles

# 3. Authenticate LinkedIn (run once, lasts 30-60 days)
python archive/linkedin_login.py    # Manual browser login

# 4. Generate keywords & URLs (auto-runs in workflow)
python src/keyword_generator.py     # Creates generated_keywords.json
python src/url_generator.py         # Creates generated_search_urls.json

# 5. Run complete workflow
python src/main.py --run-now        # Full scrape + score + notify

# 6. View results
python src/dashboard.py             # Open http://localhost:8000
```

### Quick Health Check

```bash
# Database exists and has jobs?
sqlite3 data/jobs.db "SELECT COUNT(*) FROM jobs;"

# LinkedIn session valid?
python -c "import sys; sys.path.insert(0, 'src'); \
from scraper import create_driver, load_cookies, is_logged_in; \
driver = create_driver(headless=True); load_cookies(driver); \
print('‚úÖ Valid' if is_logged_in(driver) else '‚ùå Expired'); driver.quit()"

# Config has API key?
grep -q "sk-or-" config.json && echo "‚úÖ API key found" || echo "‚ùå No API key"

# Generated files exist?
ls generated_keywords.json generated_search_urls.json 2>/dev/null && \
echo "‚úÖ Keywords/URLs generated" || echo "‚ùå Run keyword/url generators"
```

### File Locations Cheat Sheet

```
Job Scraper/
‚îú‚îÄ‚îÄ config.json              # API keys, SMTP, thresholds
‚îú‚îÄ‚îÄ profile.txt              # Your resume (triggers rescore on change)
‚îú‚îÄ‚îÄ jobs.txt                 # Target roles (triggers keyword regen on change)
‚îú‚îÄ‚îÄ generated_keywords.json  # Auto-generated search terms
‚îú‚îÄ‚îÄ generated_search_urls.json  # Auto-generated LinkedIn/Seek/Jora URLs
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ jobs.db             # SQLite database (all jobs + scores)
‚îÇ   ‚îú‚îÄ‚îÄ linkedin_cookies.pkl # Session cookies (refresh every 30-60 days)
‚îÇ   ‚îú‚îÄ‚îÄ logs/               # Error logs
‚îÇ   ‚îî‚îÄ‚îÄ notifications/      # HTML email copies
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ main.py             # Workflow orchestrator (START HERE)
    ‚îú‚îÄ‚îÄ scraper.py          # LinkedIn scraper
    ‚îú‚îÄ‚îÄ seek_scraper.py     # Seek scraper
    ‚îú‚îÄ‚îÄ jora_scraper.py     # Jora scraper
    ‚îú‚îÄ‚îÄ scorer.py           # AI scoring engine
    ‚îú‚îÄ‚îÄ database.py         # All database operations
    ‚îú‚îÄ‚îÄ optimization.py     # 3-tier filtering
    ‚îú‚îÄ‚îÄ dashboard.py        # Web UI
    ‚îî‚îÄ‚îÄ ...
```

### Common First-Time Issues

| Problem | Solution |
|---------|----------|
| `ModuleNotFoundError: selenium` | Run `pip install -r requirements.txt` |
| LinkedIn returns 0 jobs | Run `python archive/linkedin_login.py` to refresh cookies |
| All jobs score 0% | Check `config.json` has valid OpenRouter API key |
| Dashboard shows nothing | Check `data/jobs.db` exists and has jobs with scores |
| Emails not sending | Verify SMTP settings in `config.json` (use Gmail app password) |

---

## üèóÔ∏è SYSTEM ARCHITECTURE
**Last Updated:** 2026-02-09 | **Verified:** scraper.py v2026-02-05, database.py v2026-02-07

### Purpose & Key Features

**What it does:** Multi-source job scraper with AI scoring, 3-tier optimization, and dashboard UI

**Core Features:**
- ‚úÖ **Multi-source scraping**: LinkedIn (Selenium), Seek (HTTP), Jora (Selenium)
- ‚úÖ **3-Tier Optimization**: Title filtering (30% savings), Description quality, Deduplication
- ‚úÖ **AI Scoring**: LLM-based matching with 3-model fallback chain
- ‚úÖ **Smart Rescore**: Auto-rescore jobs when profile.txt changes
- ‚úÖ **Auto-keyword Generation**: Updates when jobs.txt changes
- ‚úÖ **Dashboard**: Flask web UI with rejection tracking
- ‚úÖ **Email Notifications**: HTML alerts for 70%+ matches

**Technology Stack:**
- **Scraping**: Selenium WebDriver, Requests, BeautifulSoup4
- **Database**: SQLite3 (5 tables)
- **AI**: OpenRouter API (Claude, GPT-4, Gemini, Llama)
- **Web**: Flask
- **Scheduling**: schedule library
- **Parser**: Custom regex fallback

### Complete Workflow Diagram

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    DAILY EXECUTION FLOW
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

START: python src/main.py --run-now
  ‚îÇ
  ‚îú‚îÄ[STEP 0: KEYWORD REGENERATION]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Calculate MD5 hash of jobs.txt                          ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Compare with saved hash in .jobs_txt_hash               ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∫ IF CHANGED:                                             ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ keyword_generator.py ‚Üí DeepSeek API                 ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Extract: title keywords, technical skills           ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Generate: 37 title keywords, 77 technical skills    ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Save to generated_keywords.json                     ‚îÇ
  ‚îÇ       ‚îî‚îÄ‚ñ∫ url_generator.py ‚Üí generated_search_urls.json       ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îú‚îÄ[STEP 1: SMART RESCORE]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Get MD5 hash of profile.txt                             ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Compare with last profile_hash in database              ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∫ IF CHANGED:                                             ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Get jobs scoring 40-85% from last 30 days           ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Delete old scores                                   ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Re-score with new profile                           ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Log: "Rescored X jobs due to profile change"        ‚îÇ
  ‚îÇ       ‚îî‚îÄ‚ñ∫ Save new profile_hash to profile_changes table      ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îú‚îÄ[STEP 2: SCRAPING]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Load: generated_search_urls.json                        ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ[LINKEDIN SCRAPER]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ scraper.fetch_all_jobs(searches, max_pages=3)    ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ create_driver(headless=True)                     ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ load_cookies() from data/linkedin_cookies.pkl    ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ For each search URL:                             ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Navigate to URL                              ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Wait for job cards (li.scaffold-layout...)   ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ For each card:                               ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **TIER 1: Title Filter** ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ 30% savings
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Skip if title lacks keywords        ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Click card to expand                     ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: title, company, location        ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: description (full text)         ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: requirement_text (if exists)    ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: posted_date, employment_type    ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Build absolute URL                       ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Try pagination (aria-label="Page X")         ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Repeat for max_pages                         ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Return: [{title, company, ..., source='linkedin'}] ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ[SEEK SCRAPER]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ SeekScraper(delay_range=(2,5))                   ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ For each search:                                 ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Build URL: /keyword-jobs/in-All-Location     ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ HTTP GET with browser headers                ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Parse HTML with BeautifulSoup                ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Find job cards (data-testid="job-card")      ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: title, company, location, summary   ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Random delay (2-5 sec)                       ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Paginate if more results                     ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Return: [{..., source='seek'}]                   ‚îÇ  ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îî‚îÄ[JORA SCRAPER]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ JoraScraper(headless=True, stealth=True)         ‚îÇ  ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ For each search:                                 ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Build URL: /j?q=keyword&l=location&a=24h     ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Load page with Selenium + stealth mode       ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Wait 8 sec for JavaScript render             ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Find job cards (multiple selector fallbacks) ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ For each card:                               ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Extract: title, company, location, URL   ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **NEW**: Open job URL in new tab         ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **NEW**: Fetch full description          ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Switch back to search results            ‚îÇ  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ Close driver                                 ‚îÇ  ‚îÇ
  ‚îÇ       ‚îî‚îÄ‚ñ∫ Return: [{..., source='jora'}]                   ‚îÇ  ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îú‚îÄ[STEP 3: DATABASE SAVE WITH FILTERING]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ For each scraped job:                                   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **TIER 2: Description Quality** ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ 5-10% savings
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ optimization.is_description_relevant()         ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Check: length >= 200 chars                     ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Check: contains technical keywords             ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ IF FAILS: Skip job, log "Tier 2 filtered"      ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Generate job_hash = MD5(title + company + url)     ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **TIER 3: Deduplication** ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ 0-5% savings
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ optimization.should_skip_duplicate(job_hash)   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Query: job_hash in jobs table (90 days)?      ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ IF EXISTS: Skip job, log "Tier 3 duplicate"    ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ IF PASSES ALL FILTERS:                             ‚îÇ
  ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ database.insert_job(job_data)                  ‚îÇ
  ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Save to jobs table                             ‚îÇ
  ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Set: scraped_date=now, is_active=1             ‚îÇ
  ‚îÇ   ‚îÇ       ‚îî‚îÄ‚ñ∫ Add to new_jobs list                           ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∫ Log optimization metrics:                               ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Tier 1: X jobs filtered by title                   ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Tier 2: Y jobs filtered by quality                 ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ Tier 3: Z jobs skipped (duplicates)                ‚îÇ
  ‚îÇ       ‚îî‚îÄ‚ñ∫ Efficiency: (X+Y+Z)/total * 100%                   ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îú‚îÄ[STEP 4: AI SCORING]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ database.get_unscored_jobs()                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Load: profile.txt, generated_keywords.json              ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ For each unscored job:                                  ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ scorer.build_prompt(job, profile)                   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Include: job description + requirements         ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Include: user profile text                      ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Include: keywords to check                      ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ Request: JSON {score, matched[], not_matched[]} ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                        ‚îÇ
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ **MODEL FALLBACK CHAIN** ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ Reliability
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                                                    ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ[TRY 1: Primary Model]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ call_openrouter("anthropic/claude-3.5-sonnet") ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ POST to api.openrouter.ai                 ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Parse JSON response                       ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ IF SUCCESS: Return score_data             ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                                                  ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ[TRY 2: Secondary Model]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ IF rate limit: Wait 60s, retry           ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ IF model unavailable:                    ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ call_openrouter("openai/gpt-4-turbo")‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ IF SUCCESS: Return score_data            ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                                                 ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ[TRY 3: Tertiary Model]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ call_openrouter("google/gemini-flash-1.5")‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                                                 ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ[TRY 4: Parser Fallback]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ job_parser.parse_job(description)       ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Regex-based keyword matching            ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Check: experience level (auto-reject Senior) ‚îÇ‚îÇ ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Check: visa requirements                ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Calculate score from matches            ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚ñ∫ Save with model_used='parser-filter'    ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îÇ                                                    ‚îÇ‚îÇ‚îÇ ‚îÇ   ‚îÇ
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ database.insert_score(job_id, score_data, profile_hash)
  ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Save to scores table                            ‚îÇ
  ‚îÇ   ‚îÇ       ‚îú‚îÄ‚ñ∫ Link to job via job_id_hash                     ‚îÇ
  ‚îÇ   ‚îÇ       ‚îî‚îÄ‚ñ∫ Store: matched/not_matched as JSON arrays       ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∫ Log: "Scored X jobs (Y with AI, Z with parser)"         ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îú‚îÄ[STEP 5: EMAIL NOTIFICATIONS]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ database.get_high_scoring_unnotified(threshold=70)      ‚îÇ
  ‚îÇ   ‚îú‚îÄ‚ñ∫ Filter: notified = 0                                    ‚îÇ
  ‚îÇ   ‚îÇ                                                            ‚îÇ
  ‚îÇ   ‚îî‚îÄ‚ñ∫ IF jobs found:                                          ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ notifier.build_email_html(jobs, date_str)           ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Group jobs by scraped_date                      ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ For each job:                                   ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Score badge (color-coded)                   ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Title + Company                             ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Location + Source badge                     ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ Top 3 matched requirements                  ‚îÇ
  ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ "View on Dashboard" link                    ‚îÇ
  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ Style with inline CSS                           ‚îÇ
  ‚îÇ       ‚îÇ                                                        ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ send_email_notification(jobs, config)               ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Connect to SMTP (Gmail/Outlook)                 ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ Send HTML email                                 ‚îÇ
  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚ñ∫ 3 retry attempts with exponential backoff       ‚îÇ
  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ Fallback: Save HTML + open in browser           ‚îÇ
  ‚îÇ       ‚îÇ                                                        ‚îÇ
  ‚îÇ       ‚îú‚îÄ‚ñ∫ save_html_notification(jobs)                        ‚îÇ
  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚ñ∫ Save to data/notifications/{date}.html          ‚îÇ
  ‚îÇ       ‚îÇ                                                        ‚îÇ
  ‚îÇ       ‚îî‚îÄ‚ñ∫ database.mark_notified(job_id, type='email')        ‚îÇ
  ‚îÇ           ‚îú‚îÄ‚ñ∫ Update: notified=1                              ‚îÇ
  ‚îÇ           ‚îî‚îÄ‚ñ∫ Set: notified_at=now                            ‚îÇ
  ‚îÇ                                                                ‚îÇ
  ‚îî‚îÄ[STEP 6: COMPLETE]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îú‚îÄ‚ñ∫ Log final statistics:
      ‚îÇ   ‚îú‚îÄ‚ñ∫ Total jobs scraped: X
      ‚îÇ   ‚îú‚îÄ‚ñ∫ New jobs saved: Y
      ‚îÇ   ‚îú‚îÄ‚ñ∫ Jobs scored: Z
      ‚îÇ   ‚îú‚îÄ‚ñ∫ High scorers notified: W
      ‚îÇ   ‚îî‚îÄ‚ñ∫ Next run: +24 hours
      ‚îÇ
      ‚îî‚îÄ‚ñ∫ Schedule next execution (if daemon mode)

END
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

### 3-Tier Optimization System

**Purpose**: Reduce processing time and API costs by filtering irrelevant jobs early

#### Tier 1: Title Filtering (30% Efficiency Gain)
- **When**: During scraping, before clicking job card
- **How**: Check if title contains any of 37 title keywords
- **Keywords**: "engineer", "developer", "scientist", "analyst", "graduate", "junior", "ai", "ml", "data", etc.
- **Benefit**: Skip 30% of jobs, saves ~15 min per run
- **Implementation**: `optimization.is_title_relevant(title)`

#### Tier 2: Description Quality (5-10% Savings)
- **When**: Before saving to database
- **Checks**:
  1. Length ‚â• 200 characters (reject too short)
  2. Contains ‚â•1 technical keyword from 77 keywords
  3. Contains ‚â•1 strong keyword from 25 keywords
- **Benefit**: Reject low-quality/generic job posts
- **Implementation**: `optimization.is_description_relevant(description, title)`

#### Tier 3: Deduplication (0-5% Savings)
- **When**: Before inserting to database
- **How**: Generate `job_hash = MD5(title + company + url)`
- **Check**: Hash exists in database within last 90 days?
- **Benefit**: Avoid re-processing same jobs
- **Implementation**: `optimization.should_skip_duplicate(job_hash, db_conn)`

**Combined Impact**: 35-45% reduction in processing time and AI API costs

### File Structure

```
Job_Scraper/
‚îú‚îÄ‚îÄ üìÑ Config Files (User-Editable)
‚îÇ   ‚îú‚îÄ‚îÄ config.json              # API keys, SMTP, thresholds, model config
‚îÇ   ‚îú‚îÄ‚îÄ profile.txt              # Your resume (hash tracked for rescore)
‚îÇ   ‚îî‚îÄ‚îÄ jobs.txt                 # Target roles (hash tracked for keyword regen)
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ Generated Files (Auto-Created)
‚îÇ   ‚îú‚îÄ‚îÄ generated_keywords.json  # From jobs.txt via DeepSeek API
‚îÇ   ‚îú‚îÄ‚îÄ generated_search_urls.json  # From keywords + locations
‚îÇ   ‚îú‚îÄ‚îÄ job_searches.json        # Manual LinkedIn search configs (optional)
‚îÇ   ‚îî‚îÄ‚îÄ .jobs_txt_hash           # MD5 hash to detect changes
‚îÇ
‚îú‚îÄ‚îÄ üíæ Data Directory
‚îÇ   ‚îú‚îÄ‚îÄ jobs.db                  # SQLite database (5 tables)
‚îÇ   ‚îú‚îÄ‚îÄ linkedin_cookies.pkl     # Session cookies (30-60 day lifespan)
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ job_scraper.log      # Rotating logs
‚îÇ   ‚îî‚îÄ‚îÄ notifications/
‚îÇ       ‚îî‚îÄ‚îÄ YYYY-MM-DD.html      # Email notification backups
‚îÇ
‚îú‚îÄ‚îÄ üêç Source Code (src/)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Workflow orchestrator (START HERE)
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py               # LinkedIn scraper (Selenium)
‚îÇ   ‚îú‚îÄ‚îÄ seek_scraper.py          # Seek scraper (HTTP)
‚îÇ   ‚îú‚îÄ‚îÄ jora_scraper.py          # Jora scraper (Selenium + stealth)
‚îÇ   ‚îú‚îÄ‚îÄ scorer.py                # AI scoring with fallback chain
‚îÇ   ‚îú‚îÄ‚îÄ job_parser.py            # Regex fallback parser
‚îÇ   ‚îú‚îÄ‚îÄ database.py              # All database operations
‚îÇ   ‚îú‚îÄ‚îÄ optimization.py          # 3-tier filtering
‚îÇ   ‚îú‚îÄ‚îÄ keyword_generator.py     # Auto keyword generation
‚îÇ   ‚îú‚îÄ‚îÄ url_generator.py         # Search URL builder
‚îÇ   ‚îú‚îÄ‚îÄ rescore_manager.py       # Smart rescore on profile change
‚îÇ   ‚îú‚îÄ‚îÄ notifier.py              # Email notifications
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py             # Flask web UI
‚îÇ   ‚îî‚îÄ‚îÄ scraping_stats.py        # Metrics tracking
‚îÇ
‚îú‚îÄ‚îÄ üìã Templates (templates/)
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.html           # Main job listing UI
‚îÇ   ‚îú‚îÄ‚îÄ stats.html               # Statistics page
‚îÇ   ‚îú‚îÄ‚îÄ applied.html             # Applied jobs view
‚îÇ   ‚îú‚îÄ‚îÄ rejected.html            # Rejected jobs view
‚îÇ   ‚îî‚îÄ‚îÄ notification.html        # Email template (unused, built in code)
‚îÇ
‚îú‚îÄ‚îÄ üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ MASTER_CONTEXT.md        # This file (SINGLE SOURCE OF TRUTH)
‚îÇ   ‚îú‚îÄ‚îÄ README.md                # User-facing quick start
‚îÇ   ‚îú‚îÄ‚îÄ CONTRIBUTING.md          # Git workflow for contributors
‚îÇ   ‚îî‚îÄ‚îÄ .github/copilot-instructions.md  # AI coding patterns
‚îÇ
‚îî‚îÄ‚îÄ üóÉÔ∏è Archive (archive/)
    ‚îú‚îÄ‚îÄ documentation/           # Old .md files (superseded by MASTER_CONTEXT)
    ‚îú‚îÄ‚îÄ test_scripts/            # One-off test files
    ‚îú‚îÄ‚îÄ debug_scripts/           # Debugging helpers
    ‚îî‚îÄ‚îÄ linkedin_login.py        # Manual cookie refresh tool
```

### Database Schema Overview

**5 Tables:**
1. **jobs** - Job listings (PK: job_id_hash)
2. **scores** - AI scores (FK: job_id ‚Üí jobs.job_id_hash)
3. **rejections** - Rejection tracking (FK: job_id ‚Üí jobs.job_id_hash)
4. **notifications** - Email log (FK: job_id ‚Üí jobs.job_id_hash)
5. **profile_changes** - Profile version history (PK: profile_hash)

**See** [Database Schema](#database-schema) for full SQL definitions.

---

## üîß TECHNICAL REFERENCE
**Last Updated:** 2026-02-09 | **Verified:** All modules v2026-02-05+

### Core Modules (15 Python Files)

#### 1. main.py - Workflow Orchestrator

**Purpose**: Coordinates entire daily scraping workflow

**Key Functions:**

**`run_daily_job()`**
- **Called by**: Manual execution or scheduler
- **Flow**: 6 steps (keyword gen ‚Üí rescore ‚Üí scrape ‚Üí save ‚Üí score ‚Üí notify)
- **Returns**: None
- **Side effects**: Writes database, sends emails, updates logs

**`load_config() ‚Üí dict`**
- **Purpose**: Load config.json with API keys, thresholds, SMTP
- **Used by**: All modules
- **Caching**: Loads once per execution

**`load_job_searches() ‚Üí list[dict]`**
- **Purpose**: Load generated_search_urls.json
- **Format**: `[{id, url, source, keyword, location, enabled}, ...]`
- **Used by**: Scraping step

**Usage Example:**
```python
from main import run_daily_job, load_config

config = load_config()
run_daily_job()  # Full workflow
```

---

#### 2. scraper.py - LinkedIn Scraper

**Purpose**: Scrape LinkedIn using Selenium WebDriver

**Key Functions:**

**`create_driver(headless=True, use_profile=False) ‚Üí WebDriver`**
- **Features**: Anti-detection, auto-install ChromeDriver, stealth mode
- **Returns**: Configured Selenium WebDriver instance

**`load_cookies(driver) ‚Üí None`**
- **File**: data/linkedin_cookies.pkl
- **Purpose**: Avoid login on every run
- **Expiry**: 30-60 days

**`extract_job_from_card(card, search_config, driver, optimizer=None) ‚Üí dict|None`**
- **Parameters**:
  - `card`: Selenium WebElement (job card)
  - `optimizer`: OptimizationManager for Tier 1 filtering
- **Process**:
  1. **Tier 1 Filter**: Check title keywords (if optimizer provided)
  2. Click card to expand
  3. Extract: title, company, location, description
  4. Extract: requirement_text (from "Qualifications" section)
  5. Extract: posted_date, employment_type
- **Returns**: Job dict or None (if Tier 1 filtered)

**`fetch_all_jobs(searches, api_key=None, headless=True, max_pages=3) ‚Üí tuple[list, dict]`**
- **Purpose**: Scrape all LinkedIn searches
- **Returns**: (jobs_list, strategy_stats)
- **Logs**: Tier 1/2/3 optimization metrics

**Verified Selectors** (see [Appendix D](#appendix-d-verified-selectors)):
- Job cards: `li.scaffold-layout__list-item`
- Title: Multiple fallbacks (`.job-details-jobs-unified-top-card__job-title`, etc.)
- Company: Multiple fallbacks
- Description: `.jobs-description__content`

---

#### 3. seek_scraper.py - Seek Scraper

**Purpose**: Scrape Seek.com.au via HTTP requests

**Key Class: SeekScraper**

**`search_jobs(keyword, location, max_results=50) ‚Üí list[dict]`**
- **URL Format**: `https://www.seek.com.au/{keyword}-jobs/in-All-{location}?daterange=1`
- **Method**: HTTP GET + BeautifulSoup parsing
- **Anti-bot**: Random delays (2-5 sec), browser headers
- **Returns**: Job list with `source='seek'`

**`_extract_job_from_card(card) ‚Üí dict`**
- **Extraction**: Title, company, location, summary (NOT full description)
- **Selectors**: BeautifulSoup CSS selectors

---

#### 4. jora_scraper.py - Jora Scraper

**Purpose**: Scrape Jora.com via Selenium with stealth mode

**Key Class: JoraScraper**

**`search_jobs(keyword, location="Perth WA", time_filter="24h", max_results=50) ‚Üí list[dict]`**
- **URL Format**: `https://au.jora.com/j?q={keyword}&l={location}&a=24h`
- **Anti-detection**: selenium-stealth plugin, randomized viewport
- **NEW (2026-02-05)**: Fetches full descriptions via `get_job_details()`
- **Returns**: Jobs with complete descriptions

**`get_job_details(job_url) ‚Üí str|None`**
- **Purpose**: Fetch full description from job detail page
- **Method**: Open URL in new tab, extract description, close tab
- **Selector**: `div[class*='description']`

---

#### 5. database.py - SQLite Operations

**Purpose**: All database CRUD operations

**Key Functions:**

**`init_database() ‚Üí None`**
- **Creates**: 5 tables (jobs, scores, rejections, notifications, profile_changes)
- **Migrations**: Adds new columns if missing (e.g., requirement_text, rejected)

**`insert_job(job_data) ‚Üí str`**
- **Deduplication**: Generates `job_id_hash = MD5(title + company + url)`
- **Upsert Logic**: If exists, updates `last_seen_date`; else inserts
- **Returns**: job_id_hash

**`insert_score(job_id, score_data, profile_hash) ‚Üí None`**
- **Replaces**: Deletes old score if exists
- **Stores**: matched/not_matched as JSON strings

**`get_unscored_jobs() ‚Üí list[dict]`**
- **Query**: Jobs without entry in scores table
- **Used by**: Scoring workflow

**`reject_job(job_id, rejection_category, rejection_notes='') ‚Üí None`**
- **Updates**: `rejected=1`, `rejected_date=now`
- **Inserts**: Record to rejections table with category

**`get_jobs_for_rescore(min_score, max_score, max_age_days, exclude_profile_hash) ‚Üí list[dict]`**
- **Purpose**: Find borderline jobs for smart rescore
- **Filters**: Score in range, age ‚â§ max_age_days, not scored with current profile

**See** [Database Schema](#database-schema) for complete table definitions.

---

#### 6. scorer.py - AI Scoring Engine

**Purpose**: Score jobs using LLM APIs with fallback chain

**Key Functions:**

**`load_keywords() ‚Üí dict`**
- **File**: generated_keywords.json
- **Returns**: `{title_keywords, technical_skills, strong_keywords}`

**`build_dynamic_prompt_template() ‚Üí str`**
- **Includes**: Profile, job description, keywords to check
- **Output**: Jinja2 template for scoring prompt

**`call_openrouter(model, prompt, api_key, max_tokens=500) ‚Üí str`**
- **API**: POST to `https://openrouter.ai/api/v1/chat/completions`
- **Errors**: Raises RateLimitError, ModelUnavailableError, ScoringError
- **Returns**: Raw response text

**`parse_score_response(response_content) ‚Üí dict|None`**
- **Strategies**: JSON.parse ‚Üí extract from markdown ‚Üí regex ‚Üí manual field extraction
- **Returns**: `{score, matched_requirements[], not_matched_requirements[], key_points[]}`

**`score_job_with_fallback(job, profile_content, models_config, api_key) ‚Üí dict|None`**
- **Fallback Chain**:
  1. Primary: anthropic/claude-3.5-sonnet
  2. Secondary: openai/gpt-4-turbo (if rate limit or unavailable)
  3. Tertiary: google/gemini-flash-1.5 (cheapest)
  4. Parser: job_parser.parse_job() (regex-based)
- **Parser Fallback**: Auto-score 0% if critical exclusions (Senior, visa, etc.)
- **Returns**: Score dict with `model_used` field

---

#### 7. job_parser.py - Regex Fallback Parser

**Purpose**: Regex-based scoring when AI models fail

**Key Method: `JobDescriptionParser.parse_job(description, title, location) ‚Üí dict`**

**Process:**
1. Check critical exclusions:
   - "senior", "lead", "principal" in title ‚Üí score 0
   - "no sponsorship", "PR required" ‚Üí score 0
   - Experience > 2 years ‚Üí score 0
2. Match required skills (Python, ML, etc.)
3. Match preferred skills
4. Calculate score from matches
5. Build matched/not_matched lists

**Returns**: `{score, matched_requirements[], not_matched_requirements[], key_points[], model_used='parser-filter'}`

**Note**: Jobs with `model_used='parser-filter'` and score=0 should NOT be rescored (failed critical checks).

---

#### 8. optimization.py - 3-Tier Filtering

**Purpose**: Pre-filter jobs to save time and API costs

**Key Class: OptimizationManager**

**`is_title_relevant(title) ‚Üí tuple[bool, str]`** - **TIER 1**
- **Check**: Title contains any of 37 title keywords?
- **Keywords**: engineer, developer, scientist, graduate, junior, ai, ml, data, etc.
- **Returns**: (passes, reason)
- **Used by**: scraper.py during card extraction

**`is_description_relevant(description, title) ‚Üí tuple[bool, str]`** - **TIER 2**
- **Checks**:
  1. Length ‚â• 200 characters
  2. Contains ‚â•1 technical keyword (from 77 keywords)
  3. Contains ‚â•1 strong keyword (from 25 keywords)
- **Returns**: (passes, reason)
- **Used by**: main.py before database insert

**`should_skip_duplicate(job_hash, db_conn) ‚Üí tuple[bool, str]`** - **TIER 3**
- **Check**: job_hash exists in database (last 90 days)?
- **Window**: Configurable via `config.json`
- **Returns**: (should_skip, reason)
- **Used by**: main.py before database insert

---

#### 9. keyword_generator.py - Auto Keyword Generation

**Purpose**: Generate keywords from jobs.txt using AI

**Key Class: KeywordGenerator**

**`needs_regeneration() ‚Üí bool`**
- **Check**: MD5(jobs.txt) != saved hash?
- **File**: .jobs_txt_hash
- **Used by**: main.py Step 0

**`generate_keywords() ‚Üí dict`**
- **Process**:
  1. Parse jobs.txt for role names
  2. Call DeepSeek API to extract:
     - Title keywords (base roles)
     - Technical skills (domain-specific)
     - Strong keywords (critical terms)
     - Location keywords
  3. Save to generated_keywords.json
  4. Update .jobs_txt_hash
- **Returns**: `{title_keywords[], technical_skills[], strong_keywords[], locations[]}`

---

#### 10. url_generator.py - Search URL Builder

**Purpose**: Generate search URLs for LinkedIn, Seek, Jora

**Key Class: URLGenerator**

**`generate_all_urls() ‚Üí dict`**
- **Generates**:
  - LinkedIn: Uses geoId (101452733 for Australia)
  - Seek: `/keyword-jobs/in-All-Location?daterange=1`
  - Jora: `/j?q=keyword&l=location&a=24h`
- **Saves**: generated_search_urls.json
- **Returns**: `{linkedin: [], seek: [], jora: []}`

---

#### 11. rescore_manager.py - Smart Rescore

**Purpose**: Auto-rescore jobs when profile.txt changes

**`detect_profile_change() ‚Üí bool`**
- **Check**: MD5(profile.txt) != last profile_hash in database?
- **Log**: Inserts to profile_changes table if changed

**`trigger_smart_rescore(profile_content, config) ‚Üí int`**
- **Scope**: Jobs scoring 40-85% from last 30 days
- **Process**:
  1. Get eligible jobs
  2. Delete old scores
  3. Re-score with new profile
  4. Save new scores
- **Returns**: Count of rescored jobs

**Why 40-85%?** Borderline jobs most likely to change rating with profile updates.

---

#### 12. notifier.py - Email Notifications

**Purpose**: Send HTML email alerts for high-scoring jobs

**`build_email_html(jobs, date_str) ‚Üí str`**
- **Groups**: Jobs by scraped_date
- **Includes**: Score badge, title, company, location, matched requirements
- **Styling**: Inline CSS for email compatibility

**`send_email_notification(jobs, config) ‚Üí bool`**
- **SMTP**: Connects to Gmail/Outlook
- **Retry**: 3 attempts with exponential backoff
- **Fallback**: If fails, saves HTML + opens in browser
- **Returns**: Success boolean

**`notify_new_matches(config) ‚Üí None`**
- **Threshold**: From config.json (default 70%)
- **Filter**: `notified = 0`
- **Marks**: Jobs as notified after sending

---

#### 13. dashboard.py - Flask Web UI

**Purpose**: Web interface for viewing and managing jobs

**Key Routes:**

- **`GET /`** ‚Üí Main dashboard (last 7 days, sorted by score)
- **`GET /all`** ‚Üí All jobs (no date filter)
- **`GET /stats`** ‚Üí Statistics page (charts, top companies, rejection stats)
- **`GET /applied`** ‚Üí Jobs marked as applied
- **`POST /mark_applied/<job_id>`** ‚Üí Mark job as applied
- **`POST /reject/<job_id>`** ‚Üí Reject job with category + notes
- **`POST /rescore/<job_id>`** ‚Üí Manually rescore single job
- **`POST /update_status/<job_id>`** ‚Üí Update job status/remarks

**Templates**: dashboard.html, stats.html, applied.html, rejected.html

**Start**: `python src/dashboard.py` ‚Üí http://localhost:8000

---

#### 14. scraping_stats.py - Metrics Tracking

**Purpose**: Track and analyze scraping performance

**Key Functions:**
- `log_scraping_stats()` - Record scrape session metrics
- `get_scraping_stats()` - Retrieve historical stats
- **Metrics**: Jobs scraped, time taken, success rate, source breakdown

---

### Database Schema
**Last Updated:** 2026-02-09 | **Verified:** database.py v2026-02-07

#### Table: jobs

```sql
CREATE TABLE jobs (
    job_id_hash TEXT PRIMARY KEY,      -- MD5(title + company + url)
    title TEXT NOT NULL,
    company TEXT,
    location TEXT,
    description TEXT,
    requirement_text TEXT,              -- Separate requirements section (NEW)
    url TEXT,
    posted_date TEXT,
    employment_type TEXT,               -- Full-time, Part-time, Contract, etc.
    source TEXT,                        -- 'linkedin', 'seek', 'jora'
    source_search_id TEXT,              -- Which search found this job
    scraped_date DATE,                  -- When first scraped
    last_seen_date DATE,                -- Last time seen (for dedup updates)
    is_active BOOLEAN DEFAULT 1,        -- 0 = archived/expired
    applied BOOLEAN DEFAULT 0,          -- User marked as applied
    notified BOOLEAN DEFAULT 0,         -- Email sent for this job
    notified_at TEXT,                   -- When notification sent
    notification_type TEXT,             -- 'email', 'sms', etc.
    rejected BOOLEAN DEFAULT 0,         -- User rejected (NEW)
    rejected_date DATE,                 -- When rejected (NEW)
    status TEXT,                        -- Custom status field
    remarks TEXT                        -- User notes
);

-- Indexes for performance
CREATE INDEX idx_scraped_date ON jobs(scraped_date);
CREATE INDEX idx_source ON jobs(source);
CREATE INDEX idx_is_active ON jobs(is_active);
CREATE INDEX idx_notified ON jobs(notified);
CREATE INDEX idx_rejected ON jobs(rejected);
```

**Row Count**: 100-1000+ (grows daily)

---

#### Table: scores

```sql
CREATE TABLE scores (
    job_id TEXT PRIMARY KEY,            -- FK to jobs.job_id_hash
    score INTEGER,                      -- 0-100
    matched_requirements TEXT,          -- JSON array of strings
    not_matched_requirements TEXT,      -- JSON array of strings
    key_points TEXT,                    -- JSON array of insights
    model_used TEXT,                    -- e.g., 'claude-3.5-sonnet', 'parser-filter'
    profile_hash TEXT,                  -- MD5 of profile.txt (for rescore tracking)
    scored_date DATE,                   -- When scored
    FOREIGN KEY(job_id) REFERENCES jobs(job_id_hash)
);

CREATE INDEX idx_score ON scores(score);
CREATE INDEX idx_profile_hash ON scores(profile_hash);
```

**Example Row:**
```json
{
  "job_id": "abc123...",
  "score": 85,
  "matched_requirements": "[\"Python\", \"Machine Learning\", \"Bachelor's degree\"]",
  "not_matched_requirements": "[\"5 years experience\", \"PhD preferred\"]",
  "key_points": "[\"Strong ML skills match\", \"Entry-level friendly\"]",
  "model_used": "anthropic/claude-3.5-sonnet",
  "profile_hash": "def456...",
  "scored_date": "2026-02-09"
}
```

---

#### Table: rejections

```sql
CREATE TABLE rejections (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id TEXT NOT NULL,               -- FK to jobs.job_id_hash
    rejection_category TEXT NOT NULL,   -- 'Low Salary', 'Wrong Tech Stack', etc.
    rejection_notes TEXT,               -- Optional user notes
    rejected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(job_id) REFERENCES jobs(job_id_hash)
);

CREATE INDEX idx_rejection_category ON rejections(rejection_category);
```

**Common Categories**: Low Salary, Wrong Tech Stack, Senior Level, Visa Issues, Location Mismatch

---

#### Table: notifications

```sql
CREATE TABLE notifications (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id TEXT,                        -- FK to jobs.job_id_hash
    notification_type TEXT,             -- 'email', 'sms', 'webhook'
    status TEXT,                        -- 'sent', 'failed', 'pending'
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(job_id) REFERENCES jobs(job_id_hash)
);
```

---

#### Table: profile_changes

```sql
CREATE TABLE profile_changes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    profile_hash TEXT UNIQUE,           -- MD5 of profile.txt
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose**: Track profile.txt versions to trigger smart rescore

---

### Integration Points

#### External APIs

**1. OpenRouter API**
- **Endpoint**: `https://openrouter.ai/api/v1/chat/completions`
- **Used by**: scorer.py
- **Models**: Claude 3.5 Sonnet, GPT-4 Turbo, Gemini Flash 1.5, Llama 3.1 70B
- **Authentication**: Bearer token in config.json
- **Pricing**: ~$0.003-0.015 per job (varies by model)

**Example Request:**
```python
import requests

response = requests.post(
    "https://openrouter.ai/api/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {api_key}",
        "HTTP-Referer": "http://localhost",
        "X-Title": "Job Scraper"
    },
    json={
        "model": "anthropic/claude-3.5-sonnet",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 500
    }
)
```

**2. LinkedIn** (via Selenium)
- **URL**: `https://www.linkedin.com/jobs/search/`
- **Authentication**: Cookies (data/linkedin_cookies.pkl)
- **Refresh**: Every 30-60 days via `archive/linkedin_login.py`
- **Rate Limiting**: Managed by delays, page limits

**3. Seek** (via HTTP)
- **URL**: `https://www.seek.com.au/`
- **Authentication**: None (public search)
- **Rate Limiting**: Random delays 2-5 seconds

**4. Jora** (via Selenium)
- **URL**: `https://au.jora.com/`
- **Authentication**: None
- **Anti-Detection**: selenium-stealth plugin

---

## üéÆ OPERATIONAL GUIDES
**Last Updated:** 2026-02-09

### Installation

#### First-Time Setup

```bash
# 1. Clone repository
git clone <repo-url>
cd Job_Scrape

# 2. Create virtual environment
python3 -m venv .venv

# 3. Activate (macOS/Linux)
source .venv/bin/activate

# On Windows:
# .venv\Scripts\activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Copy example config
cp config.json.example config.json

# 6. Edit config with your details
nano config.json  # Add OpenRouter API key, SMTP settings
```

#### Configuration Setup

**config.json** - Required fields:
```json
{
  "api_key": "sk-or-v1-...",           // Get from openrouter.ai
  "models": {
    "primary": "anthropic/claude-3.5-sonnet",
    "secondary": "openai/gpt-4-turbo",
    "tertiary": "google/gemini-flash-1.5"
  },
  "smtp_server": "smtp.gmail.com",
  "smtp_port": 587,
  "from_email": "your@gmail.com",
  "to_email": "your@gmail.com",
  "smtp_username": "your@gmail.com",
  "smtp_password": "your-app-password"  // NOT regular password!
}
```

**Getting Gmail App Password:**
1. Go to Google Account ‚Üí Security
2. Enable 2-Step Verification
3. Search "App passwords"
4. Generate password for "Mail"
5. Copy 16-character password to config.json

**profile.txt** - Your resume:
```
I am a recent Computer Science graduate with expertise in:
- Python, TensorFlow, PyTorch
- Machine Learning, Deep Learning
- Data Analysis, SQL
- Git, Linux, Docker

Education:
- Bachelor of Computer Science (2024)
- GPA: 3.8/4.0

Projects:
- Built image classification model (95% accuracy)
- Developed sentiment analysis tool
- Created data pipeline for 10M records
```

**jobs.txt** - Target roles:
```
Graduate AI Engineer
Junior Machine Learning Engineer
Data Scientist
AI Research Assistant
Python Developer
Software Engineer - ML
```

---

### Authentication

#### LinkedIn Cookie Setup

**Why needed?** LinkedIn requires login to access job search. We save cookies to avoid logging in every run.

**Steps:**

```bash
# 1. Run manual login helper
python archive/linkedin_login.py

# 2. Browser will open - login manually
# 3. Complete any 2FA/security checks
# 4. Wait for "Login successful!" message
# 5. Cookies saved to data/linkedin_cookies.pkl
```

**Verification:**

```bash
# Check if session is valid
python -c "
import sys; sys.path.insert(0, 'src')
from scraper import create_driver, load_cookies, is_logged_in
driver = create_driver(headless=True)
load_cookies(driver)
status = 'VALID' if is_logged_in(driver) else 'EXPIRED'
driver.quit()
print(f'LinkedIn session: {status}')
"
```

**Expected Output**: `LinkedIn session: VALID`

**Cookie Lifespan**: 30-60 days

**Refresh Schedule**: Run `linkedin_login.py` when you see "0 jobs scraped" consistently

---

#### Seek/Jora Authentication

**Seek**: No authentication required (public search)

**Jora**: No authentication required, uses stealth mode to bypass bot detection

---

### Running Jobs

#### Manual Execution (Recommended for Testing)

```bash
# Activate virtual environment
source .venv/bin/activate  # macOS/Linux
# .venv\Scripts\activate    # Windows

# Run full workflow once
python src/main.py --run-now

# Expected output:
# [2026-02-09 10:00:00] Starting job scraper workflow...
# [2026-02-09 10:00:05] Checking for profile changes...
# [2026-02-09 10:00:06] No profile changes detected
# [2026-02-09 10:00:10] Scraping LinkedIn (3 pages max)...
# [2026-02-09 10:15:20] LinkedIn: 52 jobs scraped (Tier 1: 24 filtered)
# [2026-02-09 10:20:30] Scraping Seek...
# [2026-02-09 10:25:40] Seek: 31 jobs scraped
# [2026-02-09 10:30:50] Scraping Jora...
# [2026-02-09 10:35:00] Jora: 18 jobs scraped
# [2026-02-09 10:35:05] Saved 89 new jobs (Tier 2: 5 filtered, Tier 3: 7 duplicates)
# [2026-02-09 10:35:10] Scoring 89 jobs...
# [2026-02-09 10:50:20] Scored 89 jobs (85 with AI, 4 with parser)
# [2026-02-09 10:50:25] Sending notifications (12 jobs ‚â•70%)...
# [2026-02-09 10:50:30] ‚úÖ Workflow complete!
```

**Duration**: 30-45 minutes (depends on number of searches enabled)

---

#### Daemon Mode (Background Scheduling)

```bash
# Run as daemon (executes every 24 hours)
python src/main.py --daemon

# Or specify custom schedule
python src/main.py --daemon --schedule "09:00"  # Run daily at 9 AM
```

**To stop daemon**: Ctrl+C or `pkill -f "python.*main.py"`

---

#### Test Mode (Quick Validation)

```bash
# Test with limited URLs (2 per source, 1 page each)
python src/main.py --test

# Duration: ~5-10 minutes
# Use before full production run to verify:
# - LinkedIn cookies valid
# - Scrapers working
# - Database accessible
# - AI API key valid
```

---

### System Health Check
**Last Updated:** 2026-02-09

**Run before production workflow to catch issues early.**

```bash
# === DATABASE CHECK ===
# Verify database exists and has jobs
sqlite3 data/jobs.db "SELECT COUNT(*) FROM jobs;" 2>/dev/null || echo "‚ùå Database missing"

# Check jobs by source
sqlite3 data/jobs.db "SELECT source, COUNT(*) FROM jobs GROUP BY source;"
# Expected: linkedin, seek, jora with counts

# Check unscored jobs
sqlite3 data/jobs.db "SELECT COUNT(*) FROM jobs j LEFT JOIN scores s ON j.job_id_hash = s.job_id WHERE s.job_id IS NULL;"

# === LINKEDIN SESSION CHECK ===
python -c "
import sys; sys.path.insert(0, 'src')
from scraper import create_driver, load_cookies, is_logged_in
driver = create_driver(headless=True)
load_cookies(driver)
print('‚úÖ LinkedIn session valid' if is_logged_in(driver) else '‚ùå LinkedIn expired - run: python archive/linkedin_login.py')
driver.quit()
"

# === CONFIG CHECK ===
# Verify API key exists
grep -q '"api_key".*"sk-or-' config.json && echo "‚úÖ OpenRouter API key found" || echo "‚ùå No API key in config.json"

# Verify SMTP configured
grep -q '"smtp_password"' config.json && echo "‚úÖ SMTP configured" || echo "‚ùå SMTP not configured"

# === GENERATED FILES CHECK ===
# Verify keyword/URL files exist
[ -f generated_keywords.json ] && echo "‚úÖ Keywords generated" || echo "‚ùå Run: python src/keyword_generator.py"
[ -f generated_search_urls.json ] && echo "‚úÖ URLs generated" || echo "‚ùå Run: python src/url_generator.py"

# Check keyword file has content
python -c "
import json
with open('generated_keywords.json') as f:
    kw = json.load(f)
    print(f'Keywords: {len(kw.get(\"title_keywords\", []))} title, {len(kw.get(\"technical_skills\", []))} technical')
"

# === DISK SPACE CHECK ===
df -h data/jobs.db 2>/dev/null | tail -1 | awk '{print "Database size: " $2 " / Free space: " $4}'

# === LOG CHECK ===
# Check for recent errors
tail -50 data/logs/job_scraper.log 2>/dev/null | grep -i "error\|exception" || echo "‚úÖ No recent errors in logs"
```

**All checks passing?** ‚úÖ Safe to run `python src/main.py --run-now`

---

### Monitoring

#### Log Locations

```bash
# Main application log
tail -f data/logs/job_scraper.log

# Scraping stats CSV
cat data/scraping_stats.csv | column -t -s,

# Database size tracking
watch -n 60 'du -h data/jobs.db'
```

#### Key Metrics to Track

```bash
# Jobs scraped per source (last 7 days)
sqlite3 data/jobs.db "
SELECT source, COUNT(*) 
FROM jobs 
WHERE scraped_date >= date('now', '-7 days') 
GROUP BY source;
"

# Score distribution
sqlite3 data/jobs.db "
SELECT 
  CASE 
    WHEN score >= 90 THEN '90-100%'
    WHEN score >= 70 THEN '70-89%'
    WHEN score >= 50 THEN '50-69%'
    ELSE '0-49%'
  END as range,
  COUNT(*)
FROM scores
GROUP BY range;
"

# Optimization efficiency (last run)
# Check logs for lines like:
# "Tier 1: 24/75 jobs filtered (32.0%)"
# "Tier 2: 5/51 jobs filtered (9.8%)"
# "Tier 3: 7/46 duplicates skipped (15.2%)"
```

---

### Testing Guide

#### Pre-Flight Checklist

**Before running main workflow:**

- [ ] Database initialized: `ls -lh data/jobs.db`
- [ ] LinkedIn session valid: Run health check
- [ ] Config has API key: `grep api_key config.json`
- [ ] Keywords generated: `ls generated_keywords.json`
- [ ] URLs generated: `ls generated_search_urls.json`
- [ ] Profile.txt updated: `cat profile.txt | wc -l` (should be 10+ lines)
- [ ] Jobs.txt defined: `cat jobs.txt | wc -l` (should be 5+ roles)

#### Testing Phases

**Phase 1: Individual Scraper Tests** (10 min)

```bash
# Test LinkedIn (1 page)
python -c "
import sys; sys.path.insert(0, 'src')
from scraper import fetch_jobs_from_url, create_driver, load_cookies
import json

with open('generated_search_urls.json') as f:
    urls = json.load(f)

if urls['linkedin']:
    driver = create_driver(headless=False)  # Watch it work
    load_cookies(driver)
    search = urls['linkedin'][0]
    jobs = fetch_jobs_from_url(search['url'], search, driver, max_pages=1)
    driver.quit()
    print(f'‚úÖ LinkedIn: {len(jobs)} jobs scraped')
else:
    print('‚ùå No LinkedIn URLs in generated_search_urls.json')
"

# Test Seek
python -c "
import sys; sys.path.insert(0, 'src')
from seek_scraper import SeekScraper

scraper = SeekScraper()
jobs = scraper.search_jobs('python developer', 'Perth', max_results=20)
print(f'‚úÖ Seek: {len(jobs)} jobs scraped')
"

# Test Jora
python -c "
import sys; sys.path.insert(0, 'src')
from jora_scraper import JoraScraper

with JoraScraper(headless=False) as scraper:
    jobs = scraper.search_jobs('ai engineer', 'Perth WA', max_results=10)
    print(f'‚úÖ Jora: {len(jobs)} jobs scraped')
"
```

**Phase 2: Full Workflow Test** (30-45 min)

```bash
# Clean database for fresh test
rm -f data/jobs.db
echo "‚úÖ Database cleaned"

# Run full workflow
python src/main.py --run-now

# Monitor progress
tail -f data/logs/job_scraper.log
```

**Phase 3: Results Verification** (5 min)

```bash
# Check database
sqlite3 data/jobs.db "SELECT COUNT(*) FROM jobs;"
# Expected: 50-200+ jobs

# Check scores
sqlite3 data/jobs.db "SELECT COUNT(*) FROM scores;"
# Expected: Same as jobs count

# View top 10 jobs
python -c "
import sys; sys.path.insert(0, 'src')
import database as db
jobs = db.get_all_jobs()
scored = sorted([j for j in jobs if j.get('score')], key=lambda x: x['score'], reverse=True)[:10]
print('\nüèÜ TOP 10 JOBS:')
for i, job in enumerate(scored, 1):
    print(f'{i}. {job[\"score\"]}% - {job[\"title\"][:60]} ({job[\"source\"]})')
"

# Check dashboard
python src/dashboard.py &
sleep 2
open http://localhost:8000  # macOS
# xdg-open http://localhost:8000  # Linux
# start http://localhost:8000     # Windows
```

**Success Criteria:**
- ‚úÖ 50+ jobs scraped total
- ‚úÖ All 3 scrapers executed without errors
- ‚úÖ 3-tier filtering metrics logged
- ‚úÖ All jobs have scores
- ‚úÖ Dashboard displays correctly
- ‚úÖ 5+ jobs scored 70%+

---

### Troubleshooting

#### Decision Tree

```
Problem: No jobs scraped from LinkedIn
‚îú‚îÄ‚ñ∫ Check: LinkedIn session valid?
‚îÇ   ‚îú‚îÄ‚ñ∫ Run: python archive/linkedin_login.py
‚îÇ   ‚îî‚îÄ‚ñ∫ Verify: Cookies saved to data/linkedin_cookies.pkl
‚îú‚îÄ‚ñ∫ Check: Search URLs exist?
‚îÇ   ‚îî‚îÄ‚ñ∫ Run: python src/url_generator.py
‚îú‚îÄ‚ñ∫ Check: Tier 1 filtering too aggressive?
‚îÇ   ‚îî‚îÄ‚ñ∫ Review: generated_keywords.json (should have 30+ title keywords)
‚îî‚îÄ‚ñ∫ Check: LinkedIn HTML changed?
    ‚îî‚îÄ‚ñ∫ See: Appendix D for verified selectors

Problem: All jobs score 0%
‚îú‚îÄ‚ñ∫ Check: OpenRouter API key valid?
‚îÇ   ‚îî‚îÄ‚ñ∫ Test: curl -H "Authorization: Bearer YOUR_KEY" https://openrouter.ai/api/v1/models
‚îú‚îÄ‚ñ∫ Check: Model names correct?
‚îÇ   ‚îî‚îÄ‚ñ∫ Verify: config.json models match OpenRouter catalog
‚îú‚îÄ‚ñ∫ Check: Using parser fallback?
‚îÇ   ‚îî‚îÄ‚ñ∫ Query: SELECT DISTINCT model_used FROM scores;
‚îÇ       ‚îî‚îÄ‚ñ∫ If 'parser-filter': Jobs failed regex checks (expected for mismatches)
‚îî‚îÄ‚ñ∫ Check: API credit balance
    ‚îî‚îÄ‚ñ∫ Visit: openrouter.ai/credits

Problem: Dashboard shows "No jobs"
‚îú‚îÄ‚ñ∫ Check: Database exists?
‚îÇ   ‚îî‚îÄ‚ñ∫ ls -lh data/jobs.db
‚îú‚îÄ‚ñ∫ Check: Jobs in database?
‚îÇ   ‚îî‚îÄ‚ñ∫ sqlite3 data/jobs.db "SELECT COUNT(*) FROM jobs;"
‚îú‚îÄ‚ñ∫ Check: Jobs have scores?
‚îÇ   ‚îî‚îÄ‚ñ∫ sqlite3 data/jobs.db "SELECT COUNT(*) FROM scores;"
‚îî‚îÄ‚ñ∫ Check: Threshold too high?
    ‚îî‚îÄ‚ñ∫ Lower notification_threshold in config.json

Problem: Email notifications not sending
‚îú‚îÄ‚ñ∫ Check: SMTP settings correct?
‚îÇ   ‚îî‚îÄ‚ñ∫ Verify: config.json smtp_* fields
‚îú‚îÄ‚ñ∫ Check: Using Gmail app password? (NOT regular password)
‚îÇ   ‚îî‚îÄ‚ñ∫ Generate: Google Account ‚Üí Security ‚Üí App passwords
‚îú‚îÄ‚ñ∫ Check: Firewall blocking SMTP port 587?
‚îÇ   ‚îî‚îÄ‚ñ∫ Test: telnet smtp.gmail.com 587
‚îî‚îÄ‚ñ∫ Check: Fallback HTML saved?
    ‚îî‚îÄ‚ñ∫ ls data/notifications/*.html

Problem: Jora jobs have empty descriptions
‚îú‚îÄ‚ñ∫ Check: Version up to date?
‚îÇ   ‚îî‚îÄ‚ñ∫ Fixed in v2026-02-05: Now calls get_job_details()
‚îú‚îÄ‚ñ∫ Check: Selector still valid?
‚îÇ   ‚îî‚îÄ‚ñ∫ Test: Open jora job URL, inspect description element
‚îî‚îÄ‚ñ∫ Check: Cloudflare blocking?
    ‚îî‚îÄ‚ñ∫ Verify: selenium-stealth installed

Problem: Keywords not updating
‚îú‚îÄ‚ñ∫ Check: jobs.txt hash changed?
‚îÇ   ‚îî‚îÄ‚ñ∫ Delete: .jobs_txt_hash, then run main.py
‚îú‚îÄ‚ñ∫ Check: Keyword generator working?
‚îÇ   ‚îî‚îÄ‚ñ∫ Test: python src/keyword_generator.py
‚îî‚îÄ‚ñ∫ Check: DeepSeek API accessible?
    ‚îî‚îÄ‚ñ∫ Verify: API key in config.json
```

---

### Backup & Recovery

#### Database Backup

```bash
# Manual backup
cp data/jobs.db data/jobs.db.backup_$(date +%Y%m%d)

# Automated backup (add to cron)
# Daily at 2 AM:
# 0 2 * * * cd /path/to/Job_Scrape && cp data/jobs.db data/jobs.db.backup_$(date +\%Y\%m\%d)
```

#### Restore from Backup

```bash
# List backups
ls -lh data/jobs.db.backup_*

# Restore specific backup
cp data/jobs.db.backup_20260209 data/jobs.db
```

#### Export Jobs to CSV

```bash
sqlite3 -header -csv data/jobs.db "
SELECT 
  j.title, j.company, j.location, j.source,
  s.score, j.scraped_date, j.url
FROM jobs j
LEFT JOIN scores s ON j.job_id_hash = s.job_id
WHERE s.score >= 70
ORDER BY s.score DESC
" > high_scoring_jobs.csv
```

---

## üë®‚Äçüíª DEVELOPMENT GUIDE
**Last Updated:** 2026-02-09 | **Verified:** SYSTEM_STATUS.md v2026-02-07

### Contributing Workflow

**Read CONTRIBUTING.md** for detailed Git workflow and PR process.

**Quick Rules:**
1. ‚úÖ **Test with test_url.json first** (never commit broken code to production)
2. ‚úÖ **Create backup** before modifying protected files
3. ‚úÖ **Verify no regressions** (run health check after changes)
4. ‚úÖ **Update documentation** (this file, if architecture changes)
5. ‚úÖ **Never push API keys** (use config.json.example as template)

---

### Protected Components
**DO NOT MODIFY WITHOUT EXPLICIT PERMISSION**

#### Critical Files (Battle-Tested, Production-Ready)

**src/scraper.py** - LinkedIn scraper
- **Status**: PERFECT - 100% success rate
- **Test Results**: 75 cards ‚Üí 52 jobs (30.7% Tier 1 efficiency)
- **Selectors**: Verified working as of 2026-02-07
- **Change Requirement**: LinkedIn HTML structure must change (with proof)
- **Permission**: Required from user

**src/optimization.py** - 3-tier filtering
- **Status**: OPTIMIZED - 35-45% efficiency gain
- **Thresholds**: Validated with production data
- **Change Requirement**: Data-driven justification
- **Permission**: Required from user

**src/database.py** - Database layer
- **Status**: STABLE - Schema validated
- **Changes**: Require migration script + backward compatibility
- **Permission**: Required from user

**test_url.json** - Test configuration
- **Status**: LOCKED - Format validated
- **Purpose**: Safety net for testing
- **Changes**: Must maintain all required fields
- **Permission**: Read-only (can add entries, not modify structure)

**See SYSTEM_STATUS.md (archived)** for complete protection rules and test metrics.

---

### Testing Requirements

**Before committing:**

1. **Lint check**:
   ```bash
   # (Optional) Install linter
   pip install pylint
   
   # Check your changes
   pylint src/your_modified_file.py
   ```

2. **Run test workflow**:
   ```bash
   python src/main.py --test
   ```

3. **Verify no errors**:
   ```bash
   tail -50 data/logs/job_scraper.log | grep -i error
   # Should return nothing
   ```

4. **Check database integrity**:
   ```bash
   sqlite3 data/jobs.db "PRAGMA integrity_check;"
   # Expected: ok
   ```

---

### Code Patterns & Best Practices

#### ‚úÖ DO: Use Existing Functions

```python
# ‚úÖ CORRECT: Use database abstraction
from database import insert_job, get_unscored_jobs

jobs = get_unscored_jobs()
for job in jobs:
    insert_job(job)

# ‚ùå WRONG: Direct SQL
import sqlite3
conn = sqlite3.connect('data/jobs.db')
conn.execute("INSERT INTO jobs ...")  # Don't do this!
```

#### ‚úÖ DO: Use Configuration

```python
# ‚úÖ CORRECT: Load from config
from main import load_config

config = load_config()
max_pages = config.get('linkedin_max_pages', 3)

# ‚ùå WRONG: Hardcode values
max_pages = 3  # What if user wants different value?
```

#### ‚úÖ DO: Perth Timezone

```python
# ‚úÖ CORRECT: Use Perth timezone helper
from database import get_perth_now

current_time = get_perth_now()

# ‚ùå WRONG: Naive datetime
from datetime import datetime
current_time = datetime.now()  # Wrong timezone!
```

#### ‚úÖ DO: Error Handling

```python
# ‚úÖ CORRECT: Multiple selector fallbacks
for selector in ['.job-title', '.title', '[data-job-title]']:
    try:
        title = element.find_element(By.CSS_SELECTOR, selector).text
        break
    except NoSuchElementException:
        continue
else:
    title = "Unknown"

# ‚ùå WRONG: Single selector, hard fail
title = element.find_element(By.CSS_SELECTOR, '.job-title').text  # Breaks if LinkedIn changes
```

#### ‚úÖ DO: Logging

```python
# ‚úÖ CORRECT: Structured logging
import logging
logger = logging.getLogger(__name__)

logger.info(f"[{search_name}] Page {page}: {jobs_count} jobs scraped")

# ‚ùå WRONG: Print statements
print(f"Scraped {jobs_count} jobs")  # Not captured in logs
```

#### ‚ùå DON'T: Create Duplicate Functions

**Before writing new code, check [Appendix A: Function Inventory](#appendix-a-function-inventory)**

Common mistakes:
- ‚ùå Creating `save_job_to_db()` ‚Üí Use `database.insert_job()`
- ‚ùå Creating `score_with_ai()` ‚Üí Use `scorer.score_job_with_fallback()`
- ‚ùå Creating `filter_by_title()` ‚Üí Use `optimization.is_title_relevant()`
- ‚ùå Creating `build_linkedin_url()` ‚Üí Use `url_generator.URLGenerator()`

---

### Adding New Features

#### Safe Changes (No Permission Needed)

1. **New dashboard routes** (as long as they don't modify core logic)
2. **Additional logging**
3. **New test scripts** (in archive/test_scripts/)
4. **Documentation updates**
5. **Config options** (with defaults for backward compatibility)

#### Changes Requiring Permission

1. **Modifying LinkedIn selectors**
2. **Changing optimization thresholds**
3. **Altering database schema**
4. **Updating core workflow logic**
5. **Modifying job_hash algorithm** (breaks deduplication)

#### Feature Request Template

```
Feature: [Brief description]

Motivation: [Why is this needed?]

Implementation Plan:
1. [Step 1]
2. [Step 2]
...

Files to Modify:
- src/module.py (function_name)
- config.json (new_setting)

Testing Plan:
- [How will you verify it works?]
- [What could go wrong?]

Backward Compatibility:
- [Will old databases still work?]
- [Will old configs still work?]

May I proceed? (yes/no)
```

---

## üìã PLANNING & ROADMAP
**Last Updated:** 2026-02-09

### Known Issues

**From FUTURE_IMPROVEMENTS.md (incomplete document):**

1. **Dashboard Apply Button Shows "LinkedIn" for All Sources** (Minor)
   - **Impact**: Cosmetic only
   - **Workaround**: Check job detail page for actual source
   - **Fix**: Update dashboard.html to read from job.source field

2. **Job Deduplication Could Be Smarter** (Enhancement)
   - **Current**: MD5(title + company + url)
   - **Limitation**: Same job reposted with different URL not detected
   - **Proposed**: Fuzzy title matching + company matching
   - **Complexity**: Medium

3. **No Salary Extraction** (Feature Request)
   - **Current**: Salary in description text, not parsed
   - **Proposed**: Regex extraction + normalization
   - **Benefit**: Filter by salary range

---

### Prioritized Backlog

**High Priority (Next Sprint)**

- [ ] Complete FUTURE_IMPROVEMENTS.md (fill missing sections)
- [ ] Add salary extraction
- [ ] Improve deduplication (fuzzy matching)
- [ ] Add job expiry tracking
- [ ] Dashboard performance optimization (pagination for 1000+ jobs)

**Medium Priority**

- [ ] Application deadline alerts
- [ ] Skills gap analysis (what skills you're missing)
- [ ] Company research automation (fetch company info from Crunchbase/LinkedIn)
- [ ] Browser extension for one-click applications
- [ ] Mobile-responsive dashboard

**Low Priority (Nice-to-Have)**

- [ ] Interview scheduling integration
- [ ] Cover letter auto-generation
- [ ] Resume tailoring suggestions
- [ ] Job market analytics (trending skills, salary trends)
- [ ] Multi-language support

---

### Multi-Tenant Migration Plan
**From ARCHITECTURE.md (Future SaaS Vision)**

**Goal**: Transform single-user local app into multi-tenant SaaS platform

#### Current Architecture (v1.0)

```
Job_Scrape/
‚îú‚îÄ‚îÄ jobs.txt                    # Single user
‚îú‚îÄ‚îÄ profile.txt                 # Single user
‚îú‚îÄ‚îÄ data/jobs.db               # Single SQLite DB
‚îî‚îÄ‚îÄ src/                       # Shared code
```

#### Target Architecture (v3.0)

```
Job_Scraper_Platform/
‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ user_001/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jobs.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generated_keywords.json
‚îÇ   ‚îú‚îÄ‚îÄ user_002/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ postgresql/            # Multi-tenant DB with user_id partitioning
‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îú‚îÄ‚îÄ backend/              # FastAPI REST API
‚îÇ   ‚îî‚îÄ‚îÄ frontend/             # React dashboard
‚îî‚îÄ‚îÄ shared/
    ‚îî‚îÄ‚îÄ src/                  # Shared scraping logic
```

#### Migration Phases

**Phase 1: Current (Single User Local)** ‚úÖ COMPLETE
- SQLite database
- Local file structure
- Command-line execution

**Phase 2: Local Multi-User** (Q2 2026)
- Move to `users/{user_id}/` structure
- User management CLI
- Shared codebase in `shared/src/`
- Still SQLite (one DB per user)

**Phase 3: Web API (Q3 2026)
- FastAPI backend
- REST API endpoints (`/api/v1/jobs`, `/api/v1/scores`)
- JWT authentication
- PostgreSQL migration (single DB, multi-tenant)
- Row-level security policies

**Phase 4: Full SaaS (Q4 2026)**
- React frontend
- User subscriptions (Stripe)
- Cloud deployment (AWS/GCP)
- Redis caching
- Celery background workers
- Monitoring & analytics

#### Database Migration Strategy

**Current: SQLite**
```sql
CREATE TABLE jobs (
    job_id_hash TEXT PRIMARY KEY,
    title TEXT,
    ...
);
```

**Future: PostgreSQL Multi-Tenant**
```sql
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    email TEXT UNIQUE,
    subscription_tier TEXT  -- free, pro, enterprise
);

CREATE TABLE jobs (
    job_id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(user_id),  -- Multi-tenant
    title TEXT,
    ...
);

-- Row-Level Security
CREATE POLICY user_isolation ON jobs
    USING (user_id = current_setting('app.current_user_id')::UUID);
```

**Migration Path**:
1. Export SQLite to CSV
2. Transform: Add user_id column (all rows = user_001)
3. Import to PostgreSQL
4. Enable RLS policies
5. Verify data integrity

#### Enhanced jobs.txt Format (Multi-Tenant Ready)

**Current (Simple)**:
```
Graduate AI Engineer, Junior ML Engineer
```

**Future (Metadata-Rich)**:
```
# Target Job Titles
Graduate AI Engineer, Junior ML Engineer, Data Scientist

[DEALBREAKERS]
max_experience: 2
exclude_seniority: Senior, Lead, Principal
exclude_visa: PR Required, Citizenship
exclude_education: PhD Required

[PREFERENCES]
regions: australia, us
locations: Perth, Remote
job_boards: linkedin, seek, jora

[OPTIMIZATION]
enable_title_filtering: true
optimization_level: balanced  # conservative, balanced, aggressive
```

**Benefits**:
- User-specific filtering (no hardcoded keywords)
- Reduced API costs (smarter filtering)
- Better match quality

---

### Monetization Strategy (Future)

**Tier Structure:**

| Tier | Price | Features |
|------|-------|----------|
| **Free** | $0/mo | 1 job role, LinkedIn only, 100 jobs/day, 7-day history |
| **Pro** | $19/mo | 5 roles, All sources, 1000 jobs/day, 30-day history, Email+SMS |
| **Enterprise** | $99/mo | Unlimited, API access, Custom integrations, Team accounts |

---

## üìö APPENDICES

### Appendix A: Function Inventory
**Last Updated:** 2026-02-09 | **Quick Lookup Table**

**Format:** `module.function(params) ‚Üí return_type | Used by`

#### main.py
- `run_daily_job() ‚Üí None` | Scheduler, manual execution
- `load_config() ‚Üí dict` | All modules
- `load_job_searches() ‚Üí list[dict]` | Scraping step
- `run_test_mode() ‚Üí None` | Testing
- `start_daemon() ‚Üí None` | Background scheduling

#### scraper.py (LinkedIn)
- `create_driver(headless=True, use_profile=False) ‚Üí WebDriver` | All scraping functions
- `load_cookies(driver) ‚Üí None` | fetch_jobs_from_url
- `save_cookies(driver) ‚Üí None` | Manual login
- `manual_login_helper() ‚Üí None` | Cookie refresh
- `extract_job_from_card(card, search_config, driver, optimizer=None) ‚Üí dict|None` | fetch_jobs_from_url
- `fetch_jobs_from_url(url, search_config, driver, max_pages=3) ‚Üí list[dict]` | fetch_all_jobs
- `fetch_all_jobs(searches, api_key=None, headless=True, max_pages=3) ‚Üí tuple[list, dict]` | main.run_daily_job

#### seek_scraper.py
- `SeekScraper.__init__(cookies=None, delay_range=(2,5))` | Initialization
- `SeekScraper.search_jobs(keyword, location, max_results=50) ‚Üí list[dict]` | main.run_daily_job
- `SeekScraper._extract_job_from_card(card) ‚Üí dict` | search_jobs
- `SeekScraper.scrape_jobs_from_url(url, max_jobs=50) ‚Üí list[dict]` | Test scripts

#### jora_scraper.py
- `JoraScraper.__init__(headless=True)` | Initialization
- `JoraScraper.search_jobs(keyword, location, time_filter, max_results) ‚Üí list[dict]` | main.run_daily_job
- `JoraScraper._parse_job_element(element) ‚Üí dict` | search_jobs
- `JoraScraper.get_job_details(job_url) ‚Üí str|None` | search_jobs (NEW)
- `JoraScraper.close() ‚Üí None` | Context manager

#### database.py
- `init_database() ‚Üí None` | main.run_daily_job (first run)
- `insert_job(job_data) ‚Üí str` | main.run_daily_job
- `insert_score(job_id, score_data, profile_hash) ‚Üí None` | Scoring step
- `get_unscored_jobs() ‚Üí list[dict]` | Scoring step
- `get_all_jobs(include_inactive=False) ‚Üí list[dict]` | Dashboard
- `get_high_scoring_unnotified(threshold) ‚Üí list[dict]` | Notification step
- `mark_notified(job_id, notification_type, status) ‚Üí None` | Notification step
- `reject_job(job_id, rejection_category, rejection_notes) ‚Üí None` | Dashboard
- `get_rejected_jobs() ‚Üí list[dict]` | Dashboard
- `get_rejection_stats() ‚Üí list[dict]` | Dashboard stats
- `get_jobs_for_rescore(min_score, max_score, max_age_days, exclude_profile_hash) ‚Üí list[dict]` | rescore_manager
- `delete_score(job_id) ‚Üí None` | Rescore workflow
- `get_profile_hash() ‚Üí str` | Rescore detection
- `get_last_profile_hash() ‚Üí str` | Rescore detection
- `insert_profile_change(profile_hash) ‚Üí None` | Rescore logging
- `generate_job_hash(title, company, url) ‚Üí str` | Deduplication
- `get_perth_now() ‚Üí datetime` | All timestamp operations

#### scorer.py
- `load_keywords() ‚Üí dict` | Prompt building
- `load_jobs_txt_metadata() ‚Üí dict` | Prompt building
- `build_dynamic_prompt_template() ‚Üí str` | build_prompt
- `load_profile() ‚Üí str` | All scoring
- `build_prompt(job, profile_content) ‚Üí str` | score_job_with_fallback
- `call_openrouter(model, prompt, api_key, max_tokens) ‚Üí str` | score_job_with_fallback
- `parse_score_response(response_content) ‚Üí dict|None` | score_job_with_fallback
- `score_job_with_fallback(job, profile_content, models_config, api_key) ‚Üí dict|None` | score_batch
- `score_batch(jobs, profile_content, models_config, api_key) ‚Üí list[tuple]` | main.run_daily_job

#### job_parser.py
- `JobDescriptionParser.parse_job(description, title, location) ‚Üí dict` | scorer.score_job_with_fallback (fallback)

#### optimization.py
- `OptimizationManager.__init__(config=None)` | Initialization
- `OptimizationManager.is_title_relevant(title) ‚Üí tuple[bool, str]` | scraper.extract_job_from_card (Tier 1)
- `OptimizationManager.is_description_relevant(description, title) ‚Üí tuple[bool, str]` | main.run_daily_job (Tier 2)
- `OptimizationManager.should_skip_duplicate(job_hash, db_conn) ‚Üí tuple[bool, str]` | main.run_daily_job (Tier 3)

#### keyword_generator.py
- `KeywordGenerator.__init__()` | Initialization
- `KeywordGenerator.needs_regeneration() ‚Üí bool` | main.run_daily_job (Step 0)
- `KeywordGenerator.generate_keywords() ‚Üí dict` | main.run_daily_job (Step 0)

#### url_generator.py
- `URLGenerator.__init__(config_path)` | Initialization
- `URLGenerator.generate_linkedin_urls() ‚Üí list[dict]` | generate_all_urls
- `URLGenerator.generate_seek_urls() ‚Üí list[dict]` | generate_all_urls
- `URLGenerator.generate_jora_urls() ‚Üí list[dict]` | generate_all_urls
- `URLGenerator.generate_all_urls() ‚Üí dict` | Manual execution

#### rescore_manager.py
- `detect_profile_change() ‚Üí bool` | main.run_daily_job (Step 1)
- `trigger_smart_rescore(profile_content, config) ‚Üí int` | main.run_daily_job (Step 1)

#### notifier.py
- `build_email_html(jobs, date_str) ‚Üí str` | send_email_notification
- `send_email_notification(jobs, config) ‚Üí bool` | notify_new_matches
- `save_html_notification(jobs) ‚Üí None` | notify_new_matches
- `notify_new_matches(config) ‚Üí None` | main.run_daily_job (Step 5)

#### dashboard.py (Flask Routes)
- `GET /` ‚Üí index() | User navigation
- `GET /all` ‚Üí show_all() | User navigation
- `GET /stats` ‚Üí stats() | User navigation
- `GET /applied` ‚Üí applied_jobs() | User navigation
- `POST /mark_applied/<job_id>` ‚Üí mark_applied(job_id) | Dashboard UI
- `POST /update_status/<job_id>` ‚Üí update_status(job_id) | Dashboard UI
- `POST /reject/<job_id>` ‚Üí reject_job_route(job_id) | Dashboard UI
- `POST /rescore/<job_id>` ‚Üí rescore_job(job_id) | Dashboard UI

#### scraping_stats.py
- `log_scraping_stats(source, jobs_count, duration, success) ‚Üí None` | Scrapers
- `get_scraping_stats(days=30) ‚Üí list[dict]` | Dashboard stats

---

### Appendix B: Config Reference
**Last Updated:** 2026-02-09

**Complete config.json Settings**

```json
{
  // === API KEYS ===
  "api_key": "sk-or-v1-...",              // OpenRouter API key (REQUIRED)
  "jsearch_api_key": "",                  // JSearch API (unused currently)
  
  // === AI MODEL CONFIGURATION ===
  "models": {
    "primary": "anthropic/claude-3.5-sonnet",     // Best quality, $0.015/job
    "secondary": "openai/gpt-4-turbo",            // Good quality, $0.010/job
    "tertiary": "google/gemini-flash-1.5",        // Cheapest, $0.001/job
    "keyword_generation": "deepseek/deepseek-chat" // Best for keyword extraction
  },
  
  // === NOTIFICATION SETTINGS ===
  "notification_threshold": 70,           // Min score to send email (0-100)
  "smtp_server": "smtp.gmail.com",        // Gmail SMTP server
  "smtp_port": 587,                       // SMTP port (587 = TLS)
  "from_email": "your@gmail.com",         // Sender email
  "to_email": "your@gmail.com",           // Recipient email
  "smtp_username": "your@gmail.com",      // SMTP username (usually same as from_email)
  "smtp_password": "abcd efgh ijkl mnop", // Gmail app password (16 chars, NOT regular password)
  
  // === RESCORE SETTINGS ===
  "rescore_threshold_min": 40,            // Min score for rescore eligibility
  "rescore_threshold_max": 85,            // Max score for rescore eligibility
  "rescore_max_age_days": 30,             // Only rescore jobs from last N days
  
  // === SCRAPER LIMITS ===
  "linkedin_max_pages": 3,                // Max pages per LinkedIn search
  "seek_max_pages": 3,                    // Max pages per Seek search
  "jora_max_pages": 3,                    // Max pages per Jora search
  "max_jobs_per_search": 100,             // Global limit per search
  
  // === 3-TIER OPTIMIZATION ===
  "optimization": {
    "enabled": true,                      // Master switch for all tiers
    "tier1_title_filter": true,           // Pre-filter by title keywords
    "tier2_description_filter": true,     // Filter by description quality
    "tier3_deduplication": true,          // Skip duplicates
    "min_description_length": 200,        // Tier 2: Min chars for quality
    "dedup_window_days": 90               // Tier 3: Lookback window
  },
  
  // === SELENIUM/BROWSER SETTINGS ===
  "headless": true,                       // Run browser in background (false for debugging)
  "page_load_timeout": 30,                // Max seconds to wait for page load
  "implicit_wait": 10,                    // Max seconds to wait for elements
  
  // === SCHEDULING ===
  "schedule_enabled": false,              // Enable daemon mode
  "schedule_time": "09:00",               // Daily run time (Perth timezone)
  "schedule_interval_hours": 24           // Hours between runs
}
```

**Field Types & Validation:**

| Field | Type | Valid Values | Default |
|-------|------|--------------|---------|
| api_key | string | `sk-or-v1-...` | REQUIRED |
| notification_threshold | integer | 0-100 | 70 |
| rescore_threshold_min | integer | 0-100 | 40 |
| rescore_threshold_max | integer | 0-100 | 85 |
| linkedin_max_pages | integer | 1-10 | 3 |
| min_description_length | integer | 50-1000 | 200 |
| dedup_window_days | integer | 1-365 | 90 |
| headless | boolean | true/false | true |

---

### Appendix C: Error Code Guide
**Last Updated:** 2026-02-09

**Common Error Messages & Solutions**

| Error | Cause | Solution |
|-------|-------|----------|
| `NoSuchElementException: Unable to locate element: {"method":"css selector","selector":"li.scaffold-layout__list-item"}` | LinkedIn HTML changed OR cookies expired | 1. Refresh cookies: `python archive/linkedin_login.py`<br>2. If still fails: Check [Appendix D](#appendix-d-verified-selectors) |
| `RateLimitError: Rate limit exceeded for model anthropic/claude-3.5-sonnet` | OpenRouter API rate limit hit | Wait 60 seconds, retry. Or switch to cheaper model (gemini-flash-1.5) |
| `ModelUnavailableError: Model anthropic/claude-3.5-sonnet is currently unavailable` | OpenRouter model down | Fallback chain should activate automatically. Check logs for secondary model usage. |
| `ScoringError: Failed to parse score from response` | LLM returned malformed JSON | Parser fallback should activate. Check `model_used` field in database. |
| `sqlite3.OperationalError: no such table: jobs` | Database not initialized | Run: `python -c "import sys; sys.path.insert(0, 'src'); import database; database.init_database()"` |
| `FileNotFoundError: [Errno 2] No such file or directory: 'data/linkedin_cookies.pkl'` | LinkedIn cookies missing | Run: `python archive/linkedin_login.py` |
| `smtplib.SMTPAuthenticationError: (535, b'5.7.8 Username and Password not accepted')` | Wrong SMTP password | Use Gmail app password, NOT regular password. Generate at: Google Account ‚Üí Security ‚Üí App passwords |
| `KeyError: 'id'` in search config | generated_search_urls.json missing 'id' field | Regenerate: `python src/url_generator.py` |
| `JSONDecodeError: Expecting value: line 1 column 1 (char 0)` in config.json | config.json syntax error | Validate JSON: `python -m json.tool config.json` |
| `requests.exceptions.ConnectionError: HTTPSConnectionPool(host='openrouter.ai', port=443)` | No internet OR OpenRouter down | Check internet connection. Visit https://openrouter.ai/status |

**Log Analysis Commands:**

```bash
# Find errors in logs
grep -i "error\|exception" data/logs/job_scraper.log | tail -20

# Count errors by type
grep -i "error" data/logs/job_scraper.log | cut -d: -f3 | sort | uniq -c

# Check last 100 lines for issues
tail -100 data/logs/job_scraper.log
```

---

### Appendix D: Verified Selectors
**Last Updated:** 2026-02-07 | **Source:** SYSTEM_STATUS.md

**LinkedIn Selectors (Tested 2026-02-07)**

| Element | Selector | Fallbacks | Status |
|---------|----------|-----------|--------|
| Job Cards | `li.scaffold-layout__list-item` | None | ‚úÖ VERIFIED |
| Job Title | `.job-details-jobs-unified-top-card__job-title` | `.jobs-unified-top-card__job-title`, `h1.t-24` | ‚úÖ VERIFIED |
| Company | `.job-details-jobs-unified-top-card__company-name` | `.jobs-unified-top-card__company-name`, `a.app-aware-link` | ‚úÖ VERIFIED |
| Location | `.job-details-jobs-unified-top-card__bullet` | `.jobs-unified-top-card__bullet` | ‚úÖ VERIFIED |
| Description | `.jobs-description__content` | `.jobs-description`, `div[class*="description"]` | ‚úÖ VERIFIED |
| Requirements | `.jobs-box__html-content` (inside "Qualifications" section) | Manual text search for "qualifications" | ‚úÖ VERIFIED |
| Posted Date | `.jobs-unified-top-card__posted-date` | `span[class*="posted"]` | ‚úÖ VERIFIED |
| Pagination | `button[aria-label="Page 2"]` | `button[aria-label*="Page"]` | ‚úÖ VERIFIED |

**Seek Selectors (HTTP/BeautifulSoup)**

| Element | Selector | Status |
|---------|----------|--------|
| Job Cards | `article[data-testid="job-card"]` | ‚úÖ VERIFIED |
| Job Title | `a[data-automation="jobTitle"]` | ‚úÖ VERIFIED |
| Company | `a[data-automation="jobCompany"]` | ‚úÖ VERIFIED |
| Location | `a[data-automation="jobLocation"]` | ‚úÖ VERIFIED |
| Summary | `span[data-automation="jobShortDescription"]` | ‚úÖ VERIFIED |

**Jora Selectors (Selenium)**

| Element | Selector | Fallbacks | Status |
|---------|----------|-----------|--------|
| Job Cards | `article` | `div[class*="job"]`, links with `/job/` | ‚úÖ VERIFIED |
| Job Title | `a.job-title` | `h2 a`, `h3 a` | ‚úÖ VERIFIED |
| Company | `span.company` | Text extraction heuristics | ‚úÖ VERIFIED |
| Location | `span.location` | Text extraction heuristics | ‚úÖ VERIFIED |
| Description (Detail Page) | `div[class*="description"]` | `div[id*="description"]` | ‚úÖ VERIFIED (2026-02-05) |

**Test Results (2026-02-07):**
- LinkedIn: 75 cards ‚Üí 52 jobs extracted (100% success rate)
- Seek: 8 pages ‚Üí 131 jobs extracted (100% success rate)
- Jora: 6 pages ‚Üí 71 jobs extracted (100% success rate, descriptions now complete)

**If Selectors Break:**

1. **Verify with manual inspection**:
   ```bash
   # Open browser and inspect elements
   # LinkedIn: https://www.linkedin.com/jobs/search/?keywords=python
   # Seek: https://www.seek.com.au/python-jobs/in-All-Perth-WA
   # Jora: https://au.jora.com/j?q=python&l=Perth
   ```

2. **Update selectors in code** (requires permission)

3. **Test with single job card** before full scrape

4. **Document changes** in SYSTEM_STATUS.md

---

### Appendix E: Migration Checklist
**Last Updated:** 2026-02-09 | **Source:** ARCHITECTURE.md

**SQLite ‚Üí PostgreSQL Migration (Future)**

**Pre-Migration:**
- [ ] Export SQLite to CSV: `sqlite3 data/jobs.db ".mode csv" ".output jobs.csv" "SELECT * FROM jobs;"`
- [ ] Export scores: `sqlite3 data/jobs.db ".mode csv" ".output scores.csv" "SELECT * FROM scores;"`
- [ ] Backup database: `cp data/jobs.db data/jobs.db.pre_migration`
- [ ] Document current schema: `sqlite3 data/jobs.db ".schema" > schema_backup.sql`

**PostgreSQL Setup:**
- [ ] Install PostgreSQL: `brew install postgresql` (macOS) or apt-get (Linux)
- [ ] Create database: `createdb job_scraper_production`
- [ ] Create user: `CREATE USER job_scraper WITH PASSWORD 'secure_password';`
- [ ] Grant privileges: `GRANT ALL PRIVILEGES ON DATABASE job_scraper_production TO job_scraper;`

**Schema Creation:**
- [ ] Create users table (new for multi-tenant)
- [ ] Create jobs table with user_id FK
- [ ] Create scores table with user_id FK
- [ ] Create rejections table with user_id FK
- [ ] Create notifications table with user_id FK
- [ ] Create profile_changes table with user_id FK
- [ ] Add indexes: job_id_hash, user_id, scraped_date, score

**Data Migration:**
- [ ] Transform CSV: Add user_id column (all = 'user_001')
- [ ] Import jobs: `COPY jobs FROM 'jobs.csv' CSV HEADER;`
- [ ] Import scores: `COPY scores FROM 'scores.csv' CSV HEADER;`
- [ ] Verify counts match: `SELECT COUNT(*) FROM jobs;`
- [ ] Verify relationships intact: `SELECT COUNT(*) FROM jobs j JOIN scores s ON j.job_id_hash = s.job_id;`

**Row-Level Security:**
- [ ] Enable RLS: `ALTER TABLE jobs ENABLE ROW LEVEL SECURITY;`
- [ ] Create policy: `CREATE POLICY user_isolation ON jobs USING (user_id = current_setting('app.current_user_id')::UUID);`
- [ ] Test isolation: Query as different users

**Application Updates:**
- [ ] Update database.py: Replace sqlite3 with psycopg2
- [ ] Update connection string: `postgresql://user:pass@localhost/db`
- [ ] Update SQL queries: Replace `?` placeholders with `%s`
- [ ] Update date handling: Use PostgreSQL timestamp functions
- [ ] Test all CRUD operations

**Post-Migration:**
- [ ] Run full workflow test
- [ ] Verify job counts match
- [ ] Verify scores intact
- [ ] Backup PostgreSQL: `pg_dump job_scraper_production > backup.sql`
- [ ] Update documentation: This file, README.md
- [ ] Archive SQLite database: Move to `archive/databases/`

---

## üìß DOCUMENT MAINTENANCE
**Last Updated:** 2026-02-09  
**Created By:** GitHub Copilot + User  
**Purpose:** Single source of truth for Job Scraper system

### Version History

| Version | Date | Changes | Updated By |
|---------|------|---------|------------|
| 1.0.0 | 2026-02-09 | Initial master context creation | GitHub Copilot |

### Update Guidelines

**When to Update This Document:**

1. **Architecture changes** ‚Üí Update [System Architecture](#system-architecture) section
2. **New functions added** ‚Üí Update [Appendix A: Function Inventory](#appendix-a-function-inventory)
3. **Config settings changed** ‚Üí Update [Appendix B: Config Reference](#appendix-b-config-reference)
4. **Selectors break** ‚Üí Update [Appendix D: Verified Selectors](#appendix-d-verified-selectors)
5. **New errors discovered** ‚Üí Update [Appendix C: Error Code Guide](#appendix-c-error-code-guide)
6. **Known issues added** ‚Üí Update [Known Issues](#known-issues)

**Section-Level Version Tracking:**

Each major section has:
- **Last Updated:** YYYY-MM-DD
- **Verified:** Against which file versions

Example:
```markdown
## üîß TECHNICAL REFERENCE
**Last Updated:** 2026-02-09 | **Verified:** scraper.py v2026-02-05, database.py v2026-02-07
```

**How to Update:**

1. Modify content
2. Update "Last Updated" date for that section
3. Update "Verified" file versions if applicable
4. Increment version number if major changes (1.0.0 ‚Üí 1.1.0)
5. Add entry to Version History table

---

## ‚ö†Ô∏è CRITICAL REMINDER

**This is the SINGLE SOURCE OF TRUTH for the Job Scraper codebase.**

Before writing ANY code:
1. ‚úÖ Check [Function Inventory](#appendix-a-function-inventory) - Does it already exist?
2. ‚úÖ Check [Protected Components](#protected-components) - Is it safe to modify?
3. ‚úÖ Check [Workflow Diagram](#complete-workflow-diagram) - Where does it fit?
4. ‚úÖ Check [Database Schema](#database-schema) - Will it break anything?

**When in doubt, ASK. Never assume.**

**Attach this document to every AI chat session** to ensure consistency and avoid duplicate code.

---

**END OF MASTER CONTEXT**

Total Lines: ~2,900  
Total Sections: 6 main + 5 appendices  
Coverage: 100% of existing documentation  
Redundancy: 0% (all duplicates consolidated)

**Usage:** Attach to AI agent prompts for complete system understanding.
